Hardwares: (1)CPU 14900KF, use all 8 physical P-cores(16 logical cores) + 0 E-cores for docker containers. (2)GPU 4070-SUPER, 12 GB memory. (3)RAM 256 GB. (4)Host system ubuntu-22.04, all containers environment in line with the codes' requirement.

Datasets: Dataset 1/2, 3/4, 5/6 by density ascending order, dataset 1, 3, 5 with bounded max_degree to expore the influence of max_degree. Dataset 7-10 in node number ascending order for scalibility test. For dataset 1-6, the training:validition:test is 4:1:1 and the distribution of training set is very similar or can cover the distribution of test(check the plots for visualization) to make sure training phase has seen many similar graphs which makes the training reasonable. Note that the test dataset of 7-10 is combine to 'one large' graph, which differs from the dataset 1-6 that contains 'many small'. And we didn't add extra edges to it, so the test dataset is a unconnected graph with many CCs. For comparison, we use the test dataset for all algs to report runtime and accuracy.

Exception handling: We deem the OOM and RuntimeError as Failure. For the OOM, we tried to reduce the batch size or other hyperparameters to avoid it. But for the RuntimeError, which may be caused by the model itself or the prediction result(inf or nan).

1. DeSCo: changed the batch size for neigh and gossip to avoid some OOM, will fail(pretrained output inf, cannot finetune or retrain because of Log2Backward0 failure in gossip training) when some node has very large degree(like the 12th graph in the Set_7, which has a node with degree 1732(1966 nodes in total) and very star-shaped. Can show that graph in paper). So, for dataset 1, 3, 5 I set the uppper bound for max degree in the graph. Other settings like epoch number and learning rate keep the same with their default. For runtime, I only counted the running time for neigh and gossip model, but they need extra time for sampling neighs, which also takes time(details can be put on website). So, the reported time is the best or upper bound efficiency they can achieve under the assumption that the input format given is sampled neighs. As another reason, the sampling phase is not in parallel(we use 8 cores). Note that the transform for generating batch training data uses multi-cores, so the setting of multi-core also speed DeSCo up.
2. ESCAPE: use all 8 cores in parallel for dataset 1-6, but for 7-10 only uses one as there is only 'one large' graph for test. Input format is edgelist, which is very easy to get and is the most common format for graph data. So, the time for getting edgelist is not counted for ESCAPE, EVOKE and MOTIVO.
3. EVOKE: use all 8 cores in parallel for dataset 1-6 with opnmp turned off. For the 'one large' grpah in dataset 7-10 we run the main program with opnmp turned on. And we find that the max usage of CPU is about 4 logical cores.
4. MOTIVO: use all 8 cores in parallel for dataset 1-6, but for 7-10 only uses one as there is only 'one large' graph for test. For number of sampling, we use max(10000, 10*number of nodes). Adaptive sampling is used.


!NOTE: If dataset Set_{i} is changed, delete all the processed data in I2GNN and ESC-GNN to update!

5. issues with ESC-GNN(family):
    - batch size issue
        - they fail (run out of memory) when we set the batch size to their default value(256) for most targets and Datasets
        -  possible solutions:
            - reduce the batch size to 128, 64, 32, 16, 8, 4, 2, 1
            - use the default batch size for the targets and Datasets that can handle it and report the failure for the others
    - hard-coded degree size:
        - they fail when the degree size is larger than 200
        - possible solutions:
            - set a larger degree size (how large?)
            - report failure
    - running time:
        - they take a long time to run
        - possible solutions:
            - on our server:
                - report the running time with the default parameters
                - use a smaller datasets
                - use less datasets
                - use a smaller number of epochs (impact on the accuracy)
                - use a smaller number of layers (impact on the accuracy)
                - use a smaller number of hops (impact on the accuracy)
            - rent a more powerful servers online and run in parallel:
                - takes thousands of dollars (4-8K HKD)
